{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Let's load the provided data into some pandas dataframs and gather some basic information about each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# read in the json files\n",
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print some information about each our files\n",
    "portfolio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data successfully loaded\n",
    "\n",
    "We've now got a peek of each of our DataFrames which have been read in. Let's gather some exploratory information about the breakdown for a few of the stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the income\n",
    "income_unavailable = sum(pd.isnull(profile['income']))\n",
    "print('Income reported: ', len(profile) - income_unavailable)\n",
    "print('Income unreported: ', income_unavailable)\n",
    "\n",
    "clean_profile = profile.dropna(axis=0)\n",
    "column_name = 'income'\n",
    "\n",
    "# Lets see an income breakdown and plot it\n",
    "ax=plt.subplots(figsize=(6,3))\n",
    "# get data by column_name and display a histogram\n",
    "ax = plt.hist(clean_profile[column_name], bins=30)\n",
    "title=f'Histogram of {column_name} among reporters'\n",
    "plt.title(title, fontsize=12)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the income\n",
    "income_unavailable = len(profile[profile['age'] == 118])\n",
    "print('age reported: ', len(profile) - income_unavailable)\n",
    "print('age unreported: ', income_unavailable)\n",
    "\n",
    "clean_profile = profile.dropna(axis=0)\n",
    "column_name = 'age'\n",
    "\n",
    "# Lets see an income breakdown and plot it\n",
    "ax=plt.subplots(figsize=(6,3))\n",
    "# get data by column_name and display a histogram\n",
    "ax = plt.hist(clean_profile[column_name], bins=30)\n",
    "title=f'Histogram of {column_name} among reporters'\n",
    "plt.title(title, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see our gender breakdown\n",
    "print('Total: ', len(profile))\n",
    "print('Women: ', len(profile[profile['gender'] == 'F']))     \n",
    "print('Men: ', len(profile[profile['gender'] == 'M']))\n",
    "print('Other: ', len(profile[profile['gender'] == 'O']))\n",
    "print('None: ', sum(pd.isnull(profile['gender'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what type events are available\n",
    "types = transcript.event.unique()\n",
    "for event in types:\n",
    "    print(event, '    \\t:\\t', len(transcript[transcript['event'] == event]))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about how to proceed with data pre-processing\n",
    "\n",
    "At this point, we're ready to start transforming our data in order to maximize the amount of usefulness we'll gain from performing the Principal Component Analysis.\n",
    "\n",
    "Something we want to be able to continue to referenece is the need for our data to be kept within terms of each customer. In order to do that, we'll have to make some modifications to the profile dataFrame and include various statistics derived from the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_stats_df(df):\n",
    "    # make a new copy of the profile dataframe\n",
    "    new_df = df\n",
    "    types = transcript.event.unique()         \n",
    "    \n",
    "    event_count_map = { 'offer received': [],\n",
    "                        'offer viewed': [],\n",
    "                        'transaction': [],\n",
    "                        'offer completed': [] }\n",
    "    \n",
    "    # Let's take a count of each user's records for each event type\n",
    "    for index, row in new_df.iterrows():    \n",
    "        pid = row['id']\n",
    "        user_events = transcript[transcript['person'] == pid]\n",
    "    \n",
    "        for event in types:\n",
    "            # Add the new column with the calculated values for each event type\n",
    "            event_count_map[event].append(len(user_events[user_events['event'] == event]))\n",
    "    \n",
    "    # Now add each column based on the results above\n",
    "    new_df['received'] = event_count_map['offer received']\n",
    "    new_df['viewed'] = event_count_map['offer viewed']\n",
    "    new_df['transactions'] = event_count_map['transaction']\n",
    "    new_df['completed'] = event_count_map['offer completed']\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = user_stats_df(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result))\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "# 1) Convert membership date to age\n",
    "\n",
    "# 2) need an offer df? columns: id, person, number_of views, initial_time_to_view, time_to_complete\n",
    "#    avg_response_time add a column for avg offer age when viewed (time(viewed) - time(recieved))\n",
    "#    avg_completion_time for avg offer age when completed (time(completed) - time(viewed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop a few columns\n",
    "We can drop a couple things. Firstly, we no longer need the id as it cannot be normalized and we no longer have a use for it. Secondly, for our early model, lets drop any users who haven't reported their age - this is manifested by a reported age of '118'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = profile[profile['age'] != 118]\n",
    "print(f'Size after dropping unreported age: {len(df)}')\n",
    "\n",
    "# TODO Turn gender into a 0123 class so it doesnt need to be dropped\n",
    "clean_df = df.drop(['id','gender'], 1)\n",
    "print('Dropped id column')\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numerical features into a normalized range, 0-1\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "# store them in this dataframe\n",
    "df_scaled=pd.DataFrame(scaler.fit_transform(clean_df.astype(float)))\n",
    "\n",
    "# get same features and profile indices\n",
    "df_scaled.columns=clean_df.columns\n",
    "df_scaled.index=clean_df.index\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session() # store the current SageMaker session\n",
    "\n",
    "# get IAM role\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get default bucket\n",
    "bucket_name = session.default_bucket()\n",
    "print(bucket_name)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define location to store model artifacts\n",
    "prefix = 'customer_profiles'\n",
    "\n",
    "output_path='s3://{}/{}/'.format(bucket_name, prefix)\n",
    "\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a PCA model\n",
    "from sagemaker import PCA\n",
    "\n",
    "# this is current features - 1\n",
    "# you'll select only a portion of these to use, later\n",
    "N_COMPONENTS=6\n",
    "\n",
    "pca_SM = PCA(role=role,\n",
    "             train_instance_count=1,\n",
    "             train_instance_type='ml.c4.xlarge',\n",
    "             output_path=output_path, # specified, above\n",
    "             num_components=N_COMPONENTS, \n",
    "             sagemaker_session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to np array\n",
    "train_data_np = df_scaled.values.astype('float32')\n",
    "\n",
    "# convert to RecordSet format\n",
    "formatted_train_data = pca_SM.record_set(train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# train the PCA mode on the formatted data\n",
    "pca_SM.fit(formatted_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = 'pca-2020-01-07-10-47-05-733' ## NEEDS TO BE UPDATED MANUALLY\n",
    "\n",
    "# where the model is saved, by default\n",
    "model_key = os.path.join(prefix, training_job_name, 'output/model.tar.gz')\n",
    "print(model_key)\n",
    "\n",
    "# download and unzip model\n",
    "boto3.resource('s3').Bucket(bucket_name).download_file(model_key, 'model.tar.gz')\n",
    "\n",
    "# unzipping as model_algo-1\n",
    "os.system('tar -zxvf model.tar.gz')\n",
    "os.system('unzip model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# loading the unzipped artifacts\n",
    "pca_model_params = mx.ndarray.load('model_algo-1')\n",
    "\n",
    "# what are the params\n",
    "print(pca_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get selected params\n",
    "s=pd.DataFrame(pca_model_params['s'].asnumpy())\n",
    "v=pd.DataFrame(pca_model_params['v'].asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at top 5 components\n",
    "n_principal_components = 5\n",
    "\n",
    "start_idx = N_COMPONENTS - n_principal_components  # 33-n\n",
    "\n",
    "# print a selection of s\n",
    "print(s.iloc[start_idx:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the explained variance for the top n principal components\n",
    "# you may assume you have access to the global var N_COMPONENTS\n",
    "def explained_variance(s, n_top_components):\n",
    "    '''Calculates the approx. data variance that n_top_components captures.\n",
    "       :param s: A dataframe of singular values for top components; \n",
    "           the top value is in the last row.\n",
    "       :param n_top_components: An integer, the number of top components to use.\n",
    "       :return: The expected data variance covered by the n_top_components.'''\n",
    "    \n",
    "    start_idx = N_COMPONENTS - n_top_components  ## 33-3 = 30, for example\n",
    "    # calculate approx variance\n",
    "    exp_variance = np.square(s.iloc[start_idx:,:]).sum()/np.square(s).sum()\n",
    "    \n",
    "    return exp_variance[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cell\n",
    "n_top_components = 4 # select a value for the number of top components\n",
    "\n",
    "# calculate the explained variance\n",
    "exp_variance = explained_variance(s, n_top_components)\n",
    "print('Explained variance: ', exp_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def display_component(v, features_list, component_num, n_weights=10):\n",
    "    \n",
    "    # get index of component (last row - component_num)\n",
    "    row_idx = N_COMPONENTS-component_num\n",
    "\n",
    "    # get the list of weights from a row in v, dataframe\n",
    "    v_1_row = v.iloc[:, row_idx]\n",
    "    v_1 = np.squeeze(v_1_row.values)\n",
    "\n",
    "    # match weights to features in profile dataframe, using list comporehension\n",
    "    comps = pd.DataFrame(list(zip(v_1, features_list)), \n",
    "                         columns=['weights', 'features'])\n",
    "\n",
    "    # we'll want to sort by the largest n_weights\n",
    "    # weights can be neg/pos and we'll sort by magnitude\n",
    "    comps['abs_weights']=comps['weights'].apply(lambda x: np.abs(x))\n",
    "    sorted_weight_data = comps.sort_values('abs_weights', ascending=False).head(n_weights)\n",
    "\n",
    "    # display using seaborn\n",
    "    ax=plt.subplots(figsize=(10,6))\n",
    "    ax=sns.barplot(data=sorted_weight_data, \n",
    "                   x=\"weights\", \n",
    "                   y=\"features\", \n",
    "                   palette=\"Blues_d\")\n",
    "    ax.set_title(\"PCA Component Makeup, Component #\" + str(component_num))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display makeup of first component\n",
    "num=2\n",
    "display_component(v, df_scaled.columns.values, component_num=num, n_weights=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## DEPLOY THE PCA MODEL\n",
    "\n",
    "# this takes a little while, around 7mins\n",
    "pca_predictor = pca_SM.deploy(initial_instance_count=1, \n",
    "                              instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass np train data to the PCA model\n",
    "train_pca = pca_predictor.predict(train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the first item in the produced training features\n",
    "data_idx = 0\n",
    "print(train_pca[data_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dimensionality-reduced data\n",
    "def create_transformed_df(train_pca, profiles_scaled, n_top_components):\n",
    "    ''' Return a dataframe of data points with component features. \n",
    "        The dataframe should be indexed by User Profile and contain component values.\n",
    "        :param train_pca: A list of pca training data, returned by a PCA model.\n",
    "        :param profiles_scaled: A dataframe of normalized, original features.\n",
    "        :param n_top_components: An integer, the number of top components to use.\n",
    "        :return: A dataframe, indexed by Profile, with n_top_component values as columns.        \n",
    "     '''\n",
    "    # create new dataframe to add data to\n",
    "    profiles_transformed=pd.DataFrame()\n",
    "\n",
    "    # for each of our new, transformed data points\n",
    "    # append the component values to the dataframe\n",
    "    for data in train_pca:\n",
    "        # get component values for each data point\n",
    "        components=data.label['projection'].float32_tensor.values\n",
    "        profiles_transformed=profiles_transformed.append([list(components)])\n",
    "\n",
    "    # index by profile, just like profiles_scaled\n",
    "    profiles_transformed.index=profiles_scaled.index\n",
    "\n",
    "    # keep only the top n components\n",
    "    start_idx = N_COMPONENTS - n_top_components\n",
    "    profiles_transformed = profiles_transformed.iloc[:,start_idx:]\n",
    "    \n",
    "    # reverse columns, component order     \n",
    "    return profiles_transformed.iloc[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify top n\n",
    "top_n = 4\n",
    "\n",
    "# call your function and create a new dataframe\n",
    "pf_transformed = create_transformed_df(train_pca, df_scaled, n_top_components=top_n)\n",
    "\n",
    "# add descriptive columns\n",
    "PCA_list=[]\n",
    "\n",
    "for i in range(top_n):\n",
    "    PCA_list.append(f'c_{i}')\n",
    "\n",
    "pf_transformed.columns=PCA_list \n",
    "\n",
    "# print result\n",
    "pf_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete predictor endpoint\n",
    "session.delete_endpoint(pca_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-MEANS model\n",
    "\n",
    "# define a KMeans estimator\n",
    "from sagemaker import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 6\n",
    "\n",
    "kmeans = KMeans(role=role,\n",
    "                train_instance_count=1,\n",
    "                train_instance_type='ml.c4.xlarge',\n",
    "                output_path=output_path, # using the same output path as was defined, earlier              \n",
    "                k=NUM_CLUSTERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the transformed dataframe into record_set data\n",
    "kmeans_train_data_np = pf_transformed.values.astype('float32')\n",
    "kmeans_formatted_data = kmeans.record_set(kmeans_train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train kmeans\n",
    "kmeans.fit(kmeans_formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# deploy the model to create a predictor\n",
    "kmeans_predictor = kmeans.deploy(initial_instance_count=1, \n",
    "                                 instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted clusters for all the kmeans training data\n",
    "cluster_info=kmeans_predictor.predict(kmeans_train_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cluster info for first data point\n",
    "data_idx = 0\n",
    "\n",
    "print('user is: ', pf_transformed.index[data_idx])\n",
    "print()\n",
    "print(cluster_info[data_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all cluster labels\n",
    "cluster_labels = [c.label['closest_cluster'].float32_tensor.values[0] for c in cluster_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count up the points in each cluster\n",
    "cluster_df = pd.DataFrame(cluster_labels)[0].value_counts()\n",
    "\n",
    "print(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another method of visualizing the distribution\n",
    "# display a histogram of cluster counts\n",
    "ax =plt.subplots(figsize=(6,3))\n",
    "ax = plt.hist(cluster_labels, bins=8,  range=(-0.5, 7.5), color='blue', rwidth=0.5)\n",
    "\n",
    "title=\"Histogram of Cluster Counts\"\n",
    "plt.title(title, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete kmeans endpoint\n",
    "session.delete_endpoint(kmeans_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can be found under 'training jobs' in sagemaker menu\n",
    "kmeans_job_name = 'kmeans-2020-01-07-11-01-18-320'\n",
    "\n",
    "model_key = os.path.join(prefix, kmeans_job_name, 'output/model.tar.gz')\n",
    "\n",
    "# download the model file\n",
    "boto3.resource('s3').Bucket(bucket_name).download_file(model_key, 'model.tar.gz')\n",
    "os.system('tar -zxvf model.tar.gz')\n",
    "os.system('unzip model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the trained kmeans params using mxnet\n",
    "kmeans_model_params = mx.ndarray.load('model_algo-1')\n",
    "\n",
    "print(kmeans_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the centroids\n",
    "cluster_centroids=pd.DataFrame(kmeans_model_params[0].asnumpy())\n",
    "cluster_centroids.columns=pf_transformed.columns\n",
    "\n",
    "display(cluster_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a heatmap in component space, using the seaborn library\n",
    "plt.figure(figsize = (12,9))\n",
    "ax = sns.heatmap(cluster_centroids.T, cmap = 'YlGnBu')\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "ax.set_title(\"Attribute Value by Centroid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do each of these components mean again?\n",
    "# let's use the display function, from above\n",
    "component_num=3\n",
    "display_component(v, df_scaled.columns.values, component_num=component_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 'labels' column to the dataframe\n",
    "pf_transformed['labels']=list(map(int, cluster_labels))\n",
    "\n",
    "# sort by cluster label 0-6\n",
    "sorted_profiles = pf_transformed.sort_values('labels', ascending=True)\n",
    "# view some pts in cluster 0\n",
    "sorted_profiles.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all profiles with label == 1\n",
    "cluster=pf_transformed[pf_transformed['labels']==2]\n",
    "cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
